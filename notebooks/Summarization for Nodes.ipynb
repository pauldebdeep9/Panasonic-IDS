{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c6e4ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node</th>\n",
       "      <th>sources</th>\n",
       "      <th>Action</th>\n",
       "      <th>Object</th>\n",
       "      <th>Location</th>\n",
       "      <th>DateTime</th>\n",
       "      <th>Actor</th>\n",
       "      <th>Main Idea</th>\n",
       "      <th>Unsure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>to get similar to the low cost available in Ch...</td>\n",
       "      <td>f-476</td>\n",
       "      <td>get</td>\n",
       "      <td>low cost</td>\n",
       "      <td>China</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>costs</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a possible power shortage in the coming winter</td>\n",
       "      <td>f-4849</td>\n",
       "      <td>NaN</td>\n",
       "      <td>power shortage</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the coming winter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>power shortage</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you do things the India way</td>\n",
       "      <td>f-29</td>\n",
       "      <td>do</td>\n",
       "      <td>the India way</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>you</td>\n",
       "      <td>do things the India way</td>\n",
       "      <td>???</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cost of vehicles does not go up steeply .</td>\n",
       "      <td>f-123</td>\n",
       "      <td>does not go up</td>\n",
       "      <td>cost of vehicles</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>costs</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>once booked , a swipe of your ID card or passp...</td>\n",
       "      <td>f-5198</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>convenience</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12759</th>\n",
       "      <td>” SOURCE : ACEA</td>\n",
       "      <td>f-1804</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12760</th>\n",
       "      <td>” Stopping work at the factory would harm Stel...</td>\n",
       "      <td>f-808</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12761</th>\n",
       "      <td>” The slowdown in China is likely to have knoc...</td>\n",
       "      <td>f-4742</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12762</th>\n",
       "      <td>” don ’ t expect to see it on Australian stree...</td>\n",
       "      <td>f-4473</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12763</th>\n",
       "      <td>… no .</td>\n",
       "      <td>f-4236</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12764 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    node sources  \\\n",
       "0      to get similar to the low cost available in Ch...   f-476   \n",
       "1         a possible power shortage in the coming winter  f-4849   \n",
       "2                            you do things the India way    f-29   \n",
       "3              cost of vehicles does not go up steeply .   f-123   \n",
       "4      once booked , a swipe of your ID card or passp...  f-5198   \n",
       "...                                                  ...     ...   \n",
       "12759                                    ” SOURCE : ACEA  f-1804   \n",
       "12760  ” Stopping work at the factory would harm Stel...   f-808   \n",
       "12761  ” The slowdown in China is likely to have knoc...  f-4742   \n",
       "12762  ” don ’ t expect to see it on Australian stree...  f-4473   \n",
       "12763                                             … no .  f-4236   \n",
       "\n",
       "               Action            Object Location           DateTime Actor  \\\n",
       "0                 get          low cost    China                NaN   NaN   \n",
       "1                 NaN    power shortage      NaN  the coming winter   NaN   \n",
       "2                  do     the India way      NaN                NaN   you   \n",
       "3      does not go up  cost of vehicles      NaN                NaN   NaN   \n",
       "4                 NaN               NaN      NaN                NaN   NaN   \n",
       "...               ...               ...      ...                ...   ...   \n",
       "12759             NaN               NaN      NaN                NaN   NaN   \n",
       "12760             NaN               NaN      NaN                NaN   NaN   \n",
       "12761             NaN               NaN      NaN                NaN   NaN   \n",
       "12762             NaN               NaN      NaN                NaN   NaN   \n",
       "12763             NaN               NaN      NaN                NaN   NaN   \n",
       "\n",
       "                     Main Idea Unsure  \n",
       "0                        costs    NaN  \n",
       "1               power shortage    NaN  \n",
       "2      do things the India way    ???  \n",
       "3                        costs    NaN  \n",
       "4                  convenience    NaN  \n",
       "...                        ...    ...  \n",
       "12759                      NaN    NaN  \n",
       "12760                      NaN    NaN  \n",
       "12761                      NaN    NaN  \n",
       "12762                      NaN    NaN  \n",
       "12763                      NaN    NaN  \n",
       "\n",
       "[12764 rows x 9 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm, trange\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "data = pd.read_csv(\"/home/fiona/79 Mass/data/node_df_spans.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cebfeab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43995/3239594273.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_set['Formatted'] = train_set['node'] + \" --> \" + train_set['Main Idea']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node</th>\n",
       "      <th>sources</th>\n",
       "      <th>Action</th>\n",
       "      <th>Object</th>\n",
       "      <th>Location</th>\n",
       "      <th>DateTime</th>\n",
       "      <th>Actor</th>\n",
       "      <th>Main Idea</th>\n",
       "      <th>Unsure</th>\n",
       "      <th>Formatted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>to get similar to the low cost available in Ch...</td>\n",
       "      <td>f-476</td>\n",
       "      <td>get</td>\n",
       "      <td>low cost</td>\n",
       "      <td>China</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>costs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>to get similar to the low cost available in Ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a possible power shortage in the coming winter</td>\n",
       "      <td>f-4849</td>\n",
       "      <td>NaN</td>\n",
       "      <td>power shortage</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the coming winter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>power shortage</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a possible power shortage in the coming winter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you do things the India way</td>\n",
       "      <td>f-29</td>\n",
       "      <td>do</td>\n",
       "      <td>the India way</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>you</td>\n",
       "      <td>do things the India way</td>\n",
       "      <td>???</td>\n",
       "      <td>you do things the India way --&gt; do things the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cost of vehicles does not go up steeply .</td>\n",
       "      <td>f-123</td>\n",
       "      <td>does not go up</td>\n",
       "      <td>cost of vehicles</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>costs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cost of vehicles does not go up steeply . --&gt; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>once booked , a swipe of your ID card or passp...</td>\n",
       "      <td>f-5198</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>convenience</td>\n",
       "      <td>NaN</td>\n",
       "      <td>once booked , a swipe of your ID card or passp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>to safeguard the reputation of a national cham...</td>\n",
       "      <td>f-5324</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>safeguard reputation</td>\n",
       "      <td>???</td>\n",
       "      <td>to safeguard the reputation of a national cham...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>from the increase</td>\n",
       "      <td>f-4124</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>increase</td>\n",
       "      <td>???</td>\n",
       "      <td>from the increase --&gt; increase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>to make sure this does not happen again</td>\n",
       "      <td>f-842</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>prevent</td>\n",
       "      <td>???</td>\n",
       "      <td>to make sure this does not happen again --&gt; pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>you ’ re into that sort of thing</td>\n",
       "      <td>f-834</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>into that sort of thing</td>\n",
       "      <td>???</td>\n",
       "      <td>you ’ re into that sort of thing --&gt; into that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>if the winter is cold</td>\n",
       "      <td>f-4849</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>winter is cold</td>\n",
       "      <td>???</td>\n",
       "      <td>if the winter is cold --&gt; winter is cold</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>176 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 node sources          Action  \\\n",
       "0   to get similar to the low cost available in Ch...   f-476             get   \n",
       "1      a possible power shortage in the coming winter  f-4849             NaN   \n",
       "2                         you do things the India way    f-29              do   \n",
       "3           cost of vehicles does not go up steeply .   f-123  does not go up   \n",
       "4   once booked , a swipe of your ID card or passp...  f-5198             NaN   \n",
       "..                                                ...     ...             ...   \n",
       "9   to safeguard the reputation of a national cham...  f-5324             NaN   \n",
       "10                                  from the increase  f-4124             NaN   \n",
       "11            to make sure this does not happen again   f-842             NaN   \n",
       "12                   you ’ re into that sort of thing   f-834             NaN   \n",
       "13                              if the winter is cold  f-4849             NaN   \n",
       "\n",
       "              Object Location           DateTime Actor  \\\n",
       "0           low cost    China                NaN   NaN   \n",
       "1     power shortage      NaN  the coming winter   NaN   \n",
       "2      the India way      NaN                NaN   you   \n",
       "3   cost of vehicles      NaN                NaN   NaN   \n",
       "4                NaN      NaN                NaN   NaN   \n",
       "..               ...      ...                ...   ...   \n",
       "9                NaN      NaN                NaN   NaN   \n",
       "10               NaN      NaN                NaN   NaN   \n",
       "11               NaN      NaN                NaN   NaN   \n",
       "12               NaN      NaN                NaN   NaN   \n",
       "13               NaN      NaN                NaN   NaN   \n",
       "\n",
       "                  Main Idea Unsure  \\\n",
       "0                     costs    NaN   \n",
       "1            power shortage    NaN   \n",
       "2   do things the India way    ???   \n",
       "3                     costs    NaN   \n",
       "4               convenience    NaN   \n",
       "..                      ...    ...   \n",
       "9      safeguard reputation    ???   \n",
       "10                 increase    ???   \n",
       "11                  prevent    ???   \n",
       "12  into that sort of thing    ???   \n",
       "13           winter is cold    ???   \n",
       "\n",
       "                                            Formatted  \n",
       "0   to get similar to the low cost available in Ch...  \n",
       "1   a possible power shortage in the coming winter...  \n",
       "2   you do things the India way --> do things the ...  \n",
       "3   cost of vehicles does not go up steeply . --> ...  \n",
       "4   once booked , a swipe of your ID card or passp...  \n",
       "..                                                ...  \n",
       "9   to safeguard the reputation of a national cham...  \n",
       "10                     from the increase --> increase  \n",
       "11  to make sure this does not happen again --> pr...  \n",
       "12  you ’ re into that sort of thing --> into that...  \n",
       "13           if the winter is cold --> winter is cold  \n",
       "\n",
       "[176 rows x 10 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUMBER_OF_DUPLICATES = 2\n",
    "\n",
    "train_set = data.dropna(subset=['Main Idea'])\n",
    "train_set['Formatted'] = train_set['node'] + \" --> \" + train_set['Main Idea']\n",
    "\n",
    "# duplicate '???'s\n",
    "dups = []\n",
    "for _ in range(NUMBER_OF_DUPLICATES):\n",
    "    dups.append(train_set[train_set['Unsure']=='???'].reset_index(drop=True))\n",
    "train_set = pd.concat([train_set]+dups, axis=0)\n",
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c42e9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cfac555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'to get similar to the low cost available in China .',\n",
       " 'summary': 'costs',\n",
       " '__index_level_0__': 0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.from_pandas(train_set.rename(columns={\n",
    "    'node': 'text',\n",
    "    'Main Idea': 'summary'\n",
    "})[['text','summary']])\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bb3a4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "86c3d5e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ba31537e5e84351b30f7ff0bb1f73ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on train dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_column = 'text'\n",
    "summary_column = 'summary'\n",
    "prefix = \"summarize: \"\n",
    "# Temporarily set max_target_length for training.\n",
    "max_source_length = 80\n",
    "max_target_length = 20\n",
    "padding = \"max_length\"\n",
    "ignore_pad_token_for_loss = False\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # remove pairs where at least one record is None\n",
    "\n",
    "    inputs, targets = [], []\n",
    "    for i in range(len(examples[text_column])):\n",
    "        if examples[text_column][i] and examples[summary_column][i]:\n",
    "            inputs.append(examples[text_column][i])\n",
    "            targets.append(examples[summary_column][i])\n",
    "\n",
    "    inputs = [prefix + inp for inp in inputs]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\" and ignore_pad_token_for_loss:\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=column_names,\n",
    "    desc=\"Running tokenizer on train dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0dcd21aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [21603, 10, 25, 310, 163, 43, 3087, 12733, 11, 272, 476, 308, 3, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [931, 42, 1643, 3703, 16, 3, 9, 824, 2625, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "918e2d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at /home/fiona/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.d67b370cd9d75f81ad4eb421ee7b8db09e0b6a6c693b8c2b423af5d7bcac6205\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/t5-small/resolve/main/pytorch_model.bin from cache at /home/fiona/.cache/huggingface/transformers/fee5a3a0ae379232608b6eed45d2d7a0d2966b9683728838412caccc41b4b0ed.ddacdc89ec88482db20c676f0861a336f3d0409f94748c209847b49529d73885\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at /home/fiona/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.d67b370cd9d75f81ad4eb421ee7b8db09e0b6a6c693b8c2b423af5d7bcac6205\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/t5-small/resolve/main/spiece.model from cache at /home/fiona/.cache/huggingface/transformers/65fc04e21f45f61430aea0c4fedffac16a4d20d78b8e6601d8d996ebefefecd2.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
      "loading file https://huggingface.co/t5-small/resolve/main/tokenizer.json from cache at /home/fiona/.cache/huggingface/transformers/06779097c78e12f47ef67ecb728810c2ae757ee0a9efe9390c6419783d99382d.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\n",
      "loading file https://huggingface.co/t5-small/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/t5-small/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/t5-small/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at /home/fiona/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.d67b370cd9d75f81ad4eb421ee7b8db09e0b6a6c693b8c2b423af5d7bcac6205\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "baffc9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1469e89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=4,6,7\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=4,6,7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "091b5963",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp half precision backend\n",
      "***** Running training *****\n",
      "  Num examples = 176\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 24\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 160\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [160/160 00:40, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=160, training_loss=0.4075294017791748, metrics={'train_runtime': 40.5243, 'train_samples_per_second': 86.861, 'train_steps_per_second': 3.948, 'total_flos': 74437990809600.0, 'train_loss': 0.4075294017791748, 'epoch': 20.0})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"outs/summarization\",\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "    learning_rate=1e-3,\n",
    "    per_device_train_batch_size=8,\n",
    "#     per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=20,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    "    generation_max_length=max_target_length\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "#     eval_dataset=tokenized_billsum[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9b1494c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /home/fiona/79 Mass/notebooks/outs/summarization\n",
      "Configuration saved in /home/fiona/79 Mass/notebooks/outs/summarization/config.json\n",
      "Model weights saved in /home/fiona/79 Mass/notebooks/outs/summarization/pytorch_model.bin\n",
      "tokenizer config file saved in /home/fiona/79 Mass/notebooks/outs/summarization/tokenizer_config.json\n",
      "Special tokens file saved in /home/fiona/79 Mass/notebooks/outs/summarization/special_tokens_map.json\n",
      "Copy vocab file to /home/fiona/79 Mass/notebooks/outs/summarization/spiece.model\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"/home/fiona/79 Mass/notebooks/outs/summarization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca94bb73",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2b4cdebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /home/fiona/79 Mass/notebooks/outs/summarization/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"/home/fiona/79 Mass/notebooks/outs/summarization\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading configuration file /home/fiona/79 Mass/notebooks/outs/summarization/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"/home/fiona/79 Mass/notebooks/outs/summarization\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file /home/fiona/79 Mass/notebooks/outs/summarization/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at /home/fiona/79 Mass/notebooks/outs/summarization.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "Didn't find file /home/fiona/79 Mass/notebooks/outs/summarization/added_tokens.json. We won't load it.\n",
      "loading file /home/fiona/79 Mass/notebooks/outs/summarization/spiece.model\n",
      "loading file /home/fiona/79 Mass/notebooks/outs/summarization/tokenizer.json\n",
      "loading file None\n",
      "loading file /home/fiona/79 Mass/notebooks/outs/summarization/special_tokens_map.json\n",
      "loading file /home/fiona/79 Mass/notebooks/outs/summarization/tokenizer_config.json\n",
      "Your max_length is set to 200, but you input_length is only 16. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'production & supply; business expansion or downsizing; product; exports & exports; export & business expansion; investments in business performance'}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"/home/fiona/79 Mass/notebooks/outs/summarization\")\n",
    "text = 'we do not continue to make sound production plans \" along with suppliers'\n",
    "summarizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "25aff1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 80, but you input_length is only 10. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=5)\n",
      "Your max_length is set to 80, but you input_length is only 16. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'desire to live and work here; desire & desire a living & stay here; dream of living and working here; to live in the midst'},\n",
       " {'summary_text': 'production & supply; business expansion or downsizing; product; exports & exports; export & business expansion; investments in business performance'}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = 'we desire to live and work here'\n",
    "text2 = 'we do not continue to make sound production plans \" along with suppliers'\n",
    "summarizer([text1, text2], max_length=max_source_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cc96dd47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 80, but you input_length is only 64. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=32)\n",
      "Your max_length is set to 80, but you input_length is only 51. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=25)\n",
      "Your max_length is set to 80, but you input_length is only 54. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=27)\n",
      "Your max_length is set to 80, but you input_length is only 59. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=29)\n",
      "Your max_length is set to 80, but you input_length is only 46. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=23)\n",
      "Your max_length is set to 80, but you input_length is only 75. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=37)\n",
      "Your max_length is set to 80, but you input_length is only 49. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=24)\n",
      "Your max_length is set to 80, but you input_length is only 59. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=29)\n",
      "Your max_length is set to 80, but you input_length is only 60. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=30)\n",
      "Your max_length is set to 80, but you input_length is only 43. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=21)\n",
      "Your max_length is set to 80, but you input_length is only 44. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=22)\n",
      "Your max_length is set to 80, but you input_length is only 59. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=29)\n",
      "Your max_length is set to 80, but you input_length is only 53. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=26)\n",
      "Your max_length is set to 80, but you input_length is only 42. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=21)\n",
      "Your max_length is set to 80, but you input_length is only 58. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=29)\n",
      "Your max_length is set to 80, but you input_length is only 44. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=22)\n",
      "Your max_length is set to 80, but you input_length is only 54. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=27)\n",
      "Your max_length is set to 80, but you input_length is only 78. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=39)\n",
      "Your max_length is set to 80, but you input_length is only 49. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=24)\n",
      "Your max_length is set to 80, but you input_length is only 48. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=24)\n",
      "Your max_length is set to 80, but you input_length is only 62. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=31)\n",
      "Your max_length is set to 80, but you input_length is only 51. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=25)\n",
      "Your max_length is set to 80, but you input_length is only 46. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=23)\n",
      "Your max_length is set to 80, but you input_length is only 79. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=39)\n",
      "Your max_length is set to 80, but you input_length is only 41. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=20)\n",
      "Your max_length is set to 80, but you input_length is only 59. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=29)\n",
      "Your max_length is set to 80, but you input_length is only 75. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=37)\n",
      "Your max_length is set to 80, but you input_length is only 46. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=23)\n",
      "Your max_length is set to 80, but you input_length is only 70. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=35)\n",
      "Your max_length is set to 80, but you input_length is only 49. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=24)\n",
      "Your max_length is set to 80, but you input_length is only 48. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=24)\n",
      "Your max_length is set to 80, but you input_length is only 45. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=22)\n",
      "Your max_length is set to 80, but you input_length is only 62. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=31)\n",
      "Your max_length is set to 80, but you input_length is only 55. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=27)\n",
      "Your max_length is set to 80, but you input_length is only 54. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=27)\n",
      "Your max_length is set to 80, but you input_length is only 59. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=29)\n",
      "Your max_length is set to 80, but you input_length is only 44. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=22)\n",
      "Your max_length is set to 80, but you input_length is only 54. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=27)\n",
      "Your max_length is set to 80, but you input_length is only 44. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=22)\n",
      "Your max_length is set to 80, but you input_length is only 44. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=22)\n",
      "Your max_length is set to 80, but you input_length is only 46. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=23)\n",
      "Your max_length is set to 80, but you input_length is only 44. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=22)\n",
      "Your max_length is set to 80, but you input_length is only 72. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=36)\n",
      "Your max_length is set to 80, but you input_length is only 57. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=28)\n",
      "Your max_length is set to 80, but you input_length is only 61. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=30)\n",
      "Your max_length is set to 80, but you input_length is only 68. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=34)\n",
      "Your max_length is set to 80, but you input_length is only 62. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=31)\n",
      "Your max_length is set to 80, but you input_length is only 48. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=24)\n",
      "Your max_length is set to 80, but you input_length is only 45. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=22)\n",
      "Your max_length is set to 80, but you input_length is only 74. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=37)\n",
      "Your max_length is set to 80, but you input_length is only 70. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=35)\n",
      "Your max_length is set to 80, but you input_length is only 67. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=33)\n",
      "Your max_length is set to 80, but you input_length is only 42. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=21)\n",
      "Your max_length is set to 80, but you input_length is only 59. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=29)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 80, but you input_length is only 55. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=27)\n",
      "Your max_length is set to 80, but you input_length is only 75. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=37)\n",
      "Your max_length is set to 80, but you input_length is only 46. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=23)\n",
      "Your max_length is set to 80, but you input_length is only 44. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=22)\n",
      "Your max_length is set to 80, but you input_length is only 50. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=25)\n",
      "Your max_length is set to 80, but you input_length is only 56. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=28)\n",
      "Your max_length is set to 80, but you input_length is only 59. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=29)\n",
      "Your max_length is set to 80, but you input_length is only 66. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=33)\n",
      "Your max_length is set to 80, but you input_length is only 66. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=33)\n",
      "Your max_length is set to 80, but you input_length is only 43. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=21)\n",
      "Your max_length is set to 80, but you input_length is only 39. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=19)\n",
      "Your max_length is set to 80, but you input_length is only 56. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=28)\n",
      "Your max_length is set to 80, but you input_length is only 38. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=19)\n",
      "Your max_length is set to 80, but you input_length is only 49. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=24)\n",
      "Your max_length is set to 80, but you input_length is only 58. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=29)\n",
      "Your max_length is set to 80, but you input_length is only 48. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=24)\n",
      "Your max_length is set to 80, but you input_length is only 45. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=22)\n",
      "Your max_length is set to 80, but you input_length is only 44. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=22)\n",
      "Your max_length is set to 80, but you input_length is only 52. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=26)\n",
      "Your max_length is set to 80, but you input_length is only 43. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=21)\n",
      "Your max_length is set to 80, but you input_length is only 41. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=20)\n",
      "Your max_length is set to 80, but you input_length is only 41. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=20)\n",
      "Your max_length is set to 80, but you input_length is only 53. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=26)\n",
      "Your max_length is set to 80, but you input_length is only 61. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=30)\n",
      "Your max_length is set to 80, but you input_length is only 48. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=24)\n",
      "Your max_length is set to 80, but you input_length is only 38. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=19)\n",
      "Your max_length is set to 80, but you input_length is only 40. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=20)\n",
      "Your max_length is set to 80, but you input_length is only 42. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=21)\n",
      "Your max_length is set to 80, but you input_length is only 57. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=28)\n",
      "Your max_length is set to 80, but you input_length is only 39. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=19)\n",
      "Your max_length is set to 80, but you input_length is only 64. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=32)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"/home/fiona/79 Mass/data/node_df_spans.csv\")\n",
    "predictions = summarizer(list(data['node']), max_length=max_source_length, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e2aaa7ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12764"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c5bccb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['pred_main_idea'] = [p['summary_text'] for p in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "95a2bfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('pred_main_idea.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88e05c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
